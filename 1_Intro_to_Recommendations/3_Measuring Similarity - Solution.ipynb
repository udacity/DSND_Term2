{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Find Your Neighbor?\n",
    "\n",
    "In neighborhood based collaborative filtering, it is incredibly important to be able to identify an individual's neighbors.  Let's look at a small dataset in order to understand, how we can use different metrics to identify close neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "import matplotlib.pyplot as plt\n",
    "import tests as t\n",
    "import helper as h\n",
    "%matplotlib inline\n",
    "\n",
    "play_data = pd.DataFrame({'x1': [-3, -2, -1, 0, 1, 2, 3], \n",
    "               'x2': [9, 4, 1, 0, 1, 4, 9],\n",
    "               'x3': [1, 2, 3, 4, 5, 6, 7],\n",
    "               'x4': [2, 5, 15, 27, 28, 30, 31]\n",
    "})\n",
    "\n",
    "#create play data dataframe\n",
    "play_data = play_data[['x1', 'x2', 'x3', 'x4']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Measures of Similarity\n",
    "\n",
    "The first metrics we will look at have similar characteristics:\n",
    "\n",
    "1. Pearson's Correlation Coefficient\n",
    "2. Spearman's Correlation Coefficient\n",
    "3. Kendall's Tau\n",
    "\n",
    "Let's take a look at each of these individually.\n",
    "\n",
    "### Pearson's Correlation\n",
    "\n",
    "First, **Pearson's correlation coefficient** is a measure related to the strength and direction of a **linear** relationship.  \n",
    "\n",
    "If we have two vectors **x** and **y**, we can compare their individual elements in the following way to calculate Pearson's correlation coefficient:\n",
    "\n",
    "$$CORR(\\textbf{x}, \\textbf{y}) = \\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum\\limits_{i=1}^{n}(x_i-\\bar{x})^2}\\sqrt{\\sum\\limits_{i=1}^{n}(y_i-\\bar{y})^2}} $$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\bar{x} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i$$\n",
    "\n",
    "1. Write a function that takes in two vectors and returns the Pearson correlation coefficient.  You can then compare your answer to the built in function in numpy by using the assert statements in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(x, y):\n",
    "    '''\n",
    "    INPUT\n",
    "    x - an array of matching length to array y\n",
    "    y - an array of matching length to array x\n",
    "    OUTPUT\n",
    "    corr - the pearson correlation coefficient for comparing x and y\n",
    "    '''\n",
    "    # Compute Mean Values\n",
    "    mean_x, mean_y = np.sum(x)/len(x), np.sum(y)/len(y) \n",
    "    \n",
    "    x_diffs = x - mean_x\n",
    "    y_diffs = y - mean_y\n",
    "    numerator = np.sum(x_diffs*y_diffs)\n",
    "    denominator = np.sqrt(np.sum(x_diffs**2))*np.sqrt(np.sum(y_diffs**2))\n",
    "        \n",
    "    corr = numerator/denominator\n",
    "                            \n",
    "    return corr                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this is all you see, it looks like you are all set!  Nice job coding up Pearson's correlation coefficient!\n"
     ]
    }
   ],
   "source": [
    "# This cell will test your function against the built in numpy function\n",
    "assert pearson_corr(play_data['x1'], play_data['x2']) == np.corrcoef(play_data['x1'], play_data['x2'])[0][1], 'Oops!  The correlation between the first two columns should be 0, but your function returned {}.'.format(pearson_corr(play_data['x1'], play_data['x2']))\n",
    "assert round(pearson_corr(play_data['x1'], play_data['x3']), 2) == np.corrcoef(play_data['x1'], play_data['x3'])[0][1], 'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'.format(np.corrcoef(play_data['x1'], play_data['x3'])[0][1], pearson_corr(play_data['x1'], play_data['x3']))\n",
    "assert round(pearson_corr(play_data['x3'], play_data['x4']), 2) == round(np.corrcoef(play_data['x3'], play_data['x4'])[0][1], 2), 'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'.format(np.corrcoef(play_data['x3'], play_data['x4'])[0][1], pearson_corr(play_data['x3'], play_data['x4']))\n",
    "print(\"If this is all you see, it looks like you are all set!  Nice job coding up Pearson's correlation coefficient!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Now that you have computed **Pearson's correlation coefficient**, use the below dictionary to identify statements that are true about **this** measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  Pearson's correlation relates to a linear relationship.  The second and third cases are examples of perfect linear relationships, where we would receive values of 1 and -1.  Only having an increase or decrease that are directly related will not lead to a Pearson's correlation coefficient of 1 or -1.  You can see this by testing out your function using the examples above without using assert statements.\n"
     ]
    }
   ],
   "source": [
    "a = True\n",
    "b = False\n",
    "c = \"We can't be sure.\"\n",
    "\n",
    "\n",
    "pearson_dct = {\"If when x increases, y always increases, Pearson's correlation will be always be 1.\": b,\n",
    "               \"If when x increases by 1, y always increases by 3, Pearson's correlation will always be 1.\": a,\n",
    "               \"If when x increases by 1, y always decreases by 5, Pearson's correlation will always be -1.\": a,\n",
    "               \"If when x increases by 1, y increases by 3 times x, Pearson's correlation will always be 1.\": b\n",
    "}\n",
    "\n",
    "t.sim_2_sol(pearson_dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman's Correlation\n",
    "\n",
    "Now, let's look at **Spearman's correlation coefficient**.  Spearman's correlation is what is known as a [non-parametric](https://en.wikipedia.org/wiki/Nonparametric_statistics) statistic, which is a statistic who's distribution doesn't depend parameters (statistics that follow normal distributions or binomial distributions are examples of parametric statistics).  \n",
    "\n",
    "Frequently non-parametric statistics are based on the ranks of data rather than the original values collected.  This happens to be the case with Spearman's correlation coefficient, which is calculated similarly to Pearson's correlation.  However, instead of using the raw data, we use the rank of each value.\n",
    "\n",
    "You can quickly change from the raw data to the ranks using the **.rank()** method as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ranked values for the variable x1 are: [ 1.  2.  3.  4.  5.  6.  7.]\n",
      "The raw data values for the variable x1 are: [-3 -2 -1  0  1  2  3]\n"
     ]
    }
   ],
   "source": [
    "print(\"The ranked values for the variable x1 are: {}\".format(np.array(play_data['x1'].rank())))\n",
    "print(\"The raw data values for the variable x1 are: {}\".format(np.array(play_data['x1'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we map each of our data to ranked data values as shown above:\n",
    "\n",
    "$$\\textbf{x} \\rightarrow \\textbf{x}^{r}$$\n",
    "$$\\textbf{y} \\rightarrow \\textbf{y}^{r}$$\n",
    "\n",
    "Here, we let the **r** indicate these are ranked values (this is not raising any value to the power of r).  Then we compute Spearman's correlation coefficient as:\n",
    "\n",
    "$$SCORR(\\textbf{x}, \\textbf{y}) = \\frac{\\sum\\limits_{i=1}^{n}(x^{r}_i - \\bar{x}^{r})(y^{r}_i - \\bar{y}^{r})}{\\sqrt{\\sum\\limits_{i=1}^{n}(x^{r}_i-\\bar{x}^{r})^2}\\sqrt{\\sum\\limits_{i=1}^{n}(y^{r}_i-\\bar{y}^{r})^2}} $$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\bar{x}^r = \\frac{1}{n}\\sum\\limits_{i=1}^{n}x^r_i$$\n",
    "\n",
    "`3.` Write a function that takes in two vectors and returns the Spearman correlation coefficient.  You can then compare your answer to the built in function in scipy stats by using the assert statements in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corr_spearman(x, y):\n",
    "    '''\n",
    "    INPUT\n",
    "    x - an array of matching length to array y\n",
    "    y - an array of matching length to array x\n",
    "    OUTPUT\n",
    "    corr - the spearman correlation coefficient for comparing x and y\n",
    "    '''\n",
    "    # Change each vector to ranked values\n",
    "    x = x.rank()\n",
    "    y = y.rank()\n",
    "    \n",
    "    # Compute Mean Values\n",
    "    mean_x, mean_y = np.sum(x)/len(x), np.sum(y)/len(y) \n",
    "    \n",
    "    x_diffs = x - mean_x\n",
    "    y_diffs = y - mean_y\n",
    "    numerator = np.sum(x_diffs*y_diffs)\n",
    "    denominator = np.sqrt(np.sum(x_diffs**2))*np.sqrt(np.sum(y_diffs**2))\n",
    "        \n",
    "    corr = numerator/denominator\n",
    "                            \n",
    "    return corr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this is all you see, it looks like you are all set!  Nice job coding up Spearman's correlation coefficient!\n"
     ]
    }
   ],
   "source": [
    "# This cell will test your function against the built in scipy function\n",
    "assert corr_spearman(play_data['x1'], play_data['x2']) == spearmanr(play_data['x1'], play_data['x2'])[0], 'Oops!  The correlation between the first two columns should be 0, but your function returned {}.'.format(compute_corr(play_data['x1'], play_data['x2']))\n",
    "assert round(corr_spearman(play_data['x1'], play_data['x3']), 2) == spearmanr(play_data['x1'], play_data['x3'])[0], 'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'.format(np.corrcoef(play_data['x1'], play_data['x3'])[0][1], compute_corr(play_data['x1'], play_data['x3']))\n",
    "assert round(corr_spearman(play_data['x3'], play_data['x4']), 2) == round(spearmanr(play_data['x3'], play_data['x4'])[0], 2), 'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'.format(np.corrcoef(play_data['x3'], play_data['x4'])[0][1], compute_corr(play_data['x3'], play_data['x4']))\n",
    "print(\"If this is all you see, it looks like you are all set!  Nice job coding up Spearman's correlation coefficient!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now that you have computed **Spearman's correlation coefficient**, use the below dictionary to identify statements that are true about **this** measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  Unlike Pearson's correlation, Spearman's correlation can have perfect relationships (1 or -1 values) that aren't linear relationships.  You will notice that neither Spearman or Pearson correlation values suggest a relation when there are quadratic relationships.\n"
     ]
    }
   ],
   "source": [
    "a = True\n",
    "b = False\n",
    "c = \"We can't be sure.\"\n",
    "\n",
    "\n",
    "spearman_dct = {\"If when x increases, y always increases, Spearman's correlation will be always be 1.\": a,\n",
    "               \"If when x increases by 1, y always increases by 3, Pearson's correlation will always be 1.\": a,\n",
    "               \"If when x increases by 1, y always decreases by 5, Pearson's correlation will always be -1.\": a,\n",
    "               \"If when x increases by 1, y increases by 3 times x, Pearson's correlation will always be 1.\": a\n",
    "}\n",
    "\n",
    "t.sim_4_sol(spearman_dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kendall's Tau\n",
    "\n",
    "Kendall's tau is quite similar to Spearman's correlation coefficient.  Both of these measures are nonparametric measures of a relationship.  Specifically both Spearman and Kendall's coefficients are calculated based on ranking data and not the raw data.  \n",
    "\n",
    "Similar to both of the previous measures, Kendall's Tau is always between -1 and 1, where -1 suggests a strong, negative relationship between two variables and 1 suggests a strong, positive relationship between two variables.\n",
    "\n",
    "Though Spearman's and Kendall's measures are very similar, there are statistical advantages to choosing Kendall's measure in that Kendall's Tau has smaller variability when using larger sample sizes.  However Spearman's measure is more computationally efficient, as Kendall's Tau is O(n^2) and Spearman's correlation is O(nLog(n)). You can find more on this topic in [this thread](https://www.researchgate.net/post/Does_Spearmans_rho_have_any_advantage_over_Kendalls_tau).\n",
    "\n",
    "Let's take a closer look at exactly how this measure is calculated.  Again, we want to map our data to ranks:\n",
    "\n",
    "$$\\textbf{x} \\rightarrow \\textbf{x}^{r}$$\n",
    "$$\\textbf{y} \\rightarrow \\textbf{y}^{r}$$\n",
    "\n",
    "Then we calculate Kendall's Tau as:\n",
    "\n",
    "$$TAU(\\textbf{x}, \\textbf{y}) = \\frac{2}{n(n -1)}\\sum_{i < j}sgn(x^r_i - x^r_j)sgn(y^r_i - y^r_j)$$\n",
    "\n",
    "Where $sgn$ takes the the sign associated with the difference in the ranked values.  An alternative way to write \n",
    "\n",
    "$$sgn(x^r_i - x^r_j)$$ \n",
    "\n",
    "is in the following way:\n",
    "\n",
    "$$\n",
    " \\begin{cases} \n",
    "      -1  & x^r_i < x^r_j \\\\\n",
    "      0 & x^r_i = x^r_j \\\\\n",
    "      1 & x^r_i > x^r_j \n",
    "   \\end{cases}\n",
    "$$\n",
    "\n",
    "Therefore the possible results of \n",
    "\n",
    "$$sgn(x^r_i - x^r_j)sgn(y^r_i - y^r_j)$$\n",
    "\n",
    "are only 1, -1, or 0, which are summed to give an idea of the propotion of times the ranks of **x** and **y** are pointed in the right direction.\n",
    "\n",
    "`5.` Write a function that takes in two vectors and returns Kendall's Tau.  You can then compare your answer to the built in function in scipy stats by using the assert statements in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kendalls_tau(x, y):\n",
    "    '''\n",
    "    INPUT\n",
    "    x - an array of matching length to array y\n",
    "    y - an array of matching length to array x\n",
    "    OUTPUT\n",
    "    tau - the kendall's tau for comparing x and y\n",
    "    '''    \n",
    "    # Change each vector to ranked values\n",
    "    x = x.rank()\n",
    "    y = y.rank()\n",
    "    n = len(x)\n",
    "     \n",
    "    sum_vals = 0\n",
    "    # Compute Mean Values\n",
    "    for i, (x_i, y_i) in enumerate(zip(x, y)):\n",
    "        for j, (x_j, y_j) in enumerate(zip(x, y)):\n",
    "            if i < j:\n",
    "                sum_vals += np.sign(x_i - x_j)*np.sign(y_i - y_j)\n",
    "                        \n",
    "    tau = 2*sum_vals/(n*(n-1))\n",
    "    \n",
    "    return tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this is all you see, it looks like you are all set!  Nice job coding up Kendall's Tau!\n"
     ]
    }
   ],
   "source": [
    "# This cell will test your function against the built in scipy function\n",
    "assert kendalls_tau(play_data['x1'], play_data['x2']) == kendalltau(play_data['x1'], play_data['x2'])[0], 'Oops!  The correlation between the first two columns should be 0, but your function returned {}.'.format(kendalls_tau(play_data['x1'], play_data['x2']))\n",
    "assert round(kendalls_tau(play_data['x1'], play_data['x3']), 2) == kendalltau(play_data['x1'], play_data['x3'])[0], 'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'.format(kendalltau(play_data['x1'], play_data['x3'])[0][1], kendalls_tau(play_data['x1'], play_data['x3']))\n",
    "assert round(kendalls_tau(play_data['x3'], play_data['x4']), 2) == round(kendalltau(play_data['x3'], play_data['x4'])[0], 2), 'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'.format(kendalltau(play_data['x3'], play_data['x4'])[0][1], kendalls_tau(play_data['x3'], play_data['x4']))\n",
    "print(\"If this is all you see, it looks like you are all set!  Nice job coding up Kendall's Tau!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6.` Use your functions (and/or your knowledge of each of the above coefficients) to accurately identify each of the below statements as True or False.  **Note:** There may be some rounding differences due to the way numbers are stored, so it is recommended that you consider comparisons to 4 or fewer decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  Pearson does not match the other two measures, as it looks specifically for linear relationships.  However, Spearman and Kenall's measures are exactly the same to one another in the cases related to play_data.\n"
     ]
    }
   ],
   "source": [
    "a = True\n",
    "b = False\n",
    "c = \"We can't be sure.\"\n",
    "\n",
    "\n",
    "corr_comp_dct = {\"For all columns of play_data, Spearman and Kendall's measures match.\": a,\n",
    "                \"For all columns of play_data, Spearman and Pearson's measures match.\": b, \n",
    "                \"For all columns of play_data, Pearson and Kendall's measures match.\": b}\n",
    "\n",
    "t.sim_6_sol(corr_comp_dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Measures\n",
    "\n",
    "Each of the above measures are considered measures of correlation.  Similarly, there are distance measures (of which there are many).  [This is a great article](http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/) on some popular distance metrics.  In this notebook, we will be looking specifically at two of these measures.  \n",
    "\n",
    "1. Euclidean Distance\n",
    "2. Manhattan Distance\n",
    "\n",
    "Different than the three measures you built functions for, these two measures take on values between 0 and potentially infinity.  Measures that are closer to 0 imply that two vectors are more similar to one another.  The larger these values become, the more dissimilar two vectors are to one another.\n",
    "\n",
    "Choosing one of these two `distance` metrics vs. one of the three `similarity` above is often a matter of personal preference, audience, and data specificities.  You will see in a bit a case where one of these measures (euclidean or manhattan distance) is optimal to using Pearson's correlation coefficient.\n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "Euclidean distance can also just be considered as straight-line distance between two vectors.\n",
    "\n",
    "For two vectors **x** and **y**, we can compute this as:\n",
    "\n",
    "$$ EUC(\\textbf{x}, \\textbf{y}) = \\sqrt{\\sum\\limits_{i=1}^{n}(x_i - y_i)^2}$$\n",
    "\n",
    "### Manhattan Distance\n",
    "\n",
    "Different from euclidean distance, Manhattan distance is a 'manhattan block' distance from one vector to another.  Therefore, you can imagine this distance as a way to compute the distance between two points when you are not able to go through buildings.\n",
    "\n",
    "Specifically, this distance is computed as:\n",
    "\n",
    "$$ MANHATTAN(\\textbf{x}, \\textbf{y}) = \\sqrt{\\sum\\limits_{i=1}^{n}|x_i - y_i|}$$\n",
    "\n",
    "Using each of the above, write a function for each to take two vectors and compute the euclidean and manhattan distances.\n",
    "\n",
    "\n",
    "<img src=\"images/distances.png\">\n",
    "\n",
    "You can see in the above image, the **blue** line gives the **Manhattan** distance, while the **green** line gives the **Euclidean** distance between two points.\n",
    "\n",
    "`7.` Use the below cell to complete a function for each distance metric.  Then test your functions against the built in values using the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eucl_dist(x, y):\n",
    "    '''\n",
    "    INPUT\n",
    "    x - an array of matching length to array y\n",
    "    y - an array of matching length to array x\n",
    "    OUTPUT\n",
    "    euc - the euclidean distance between x and y\n",
    "    '''  \n",
    "    return np.linalg.norm(x - y)\n",
    "    \n",
    "def manhat_dist(x, y):\n",
    "    '''\n",
    "    INPUT\n",
    "    x - an array of matching length to array y\n",
    "    y - an array of matching length to array x\n",
    "    OUTPUT\n",
    "    manhat - the manhattan distance between x and y\n",
    "    '''  \n",
    "    return sum(abs(e - s) for s, e in zip(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your functions\n",
    "assert h.test_eucl(play_data['x1'], play_data['x2']) == eucl_dist(play_data['x1'], play_data['x2'])\n",
    "assert h.test_eucl(play_data['x2'], play_data['x3']) == eucl_dist(play_data['x2'], play_data['x3'])\n",
    "assert h.test_manhat(play_data['x1'], play_data['x2']) == manhat_dist(play_data['x1'], play_data['x2'])\n",
    "assert h.test_manhat(play_data['x2'], play_data['x3']) == manhat_dist(play_data['x2'], play_data['x3'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Note\n",
    "\n",
    "It is worth noting that two vectors could be similar by metrics like the three at the top of the notebook, while being incredibly, incredibly different by measures like these final two.  Again, understanding your specific situation will assist in understanding whether your metric is approporate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
