{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Parametric Tests Part II\n",
    "\n",
    "Up until now, you've been using standard hypothesis tests on means of normal distributions to design and analyze experiments. However, it's possible that you might encounter scenarios where you can't rely on only standard tests. This might be due to uncertainty about the true variability of a metric's distribution, a lack of data to assume normality, or wanting to do inference on a statistic that lacks a standard test. It's useful to know about some **non-parametric tests** not just as a workaround for cases like this, but also as a second check on your experimental results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank-Sum Test (Mann-Whitney)\n",
    "\n",
    "The rank-sum test is fairly different from the two previous approaches. There's no resamplng involved; the test is performed only on the data present. The rank-sum test, also known as the Mann-Whitney U test, is not a test of any particular statistic, like the mean or median. Instead, it's a test of distributions: let's say we draw one value at random from the populations behind each group. The null hypothesis says that there's an equal chance that the larger value is from the first group as the second group; the alternative hypothesis says that there's an unequal chance, which can be specified as one- or two-tailed.\n",
    "\n",
    "In order to test this hypothesis, we should look at the data we've collected and see in how many cases values from one group win compared to values in the second. That is, for each data point in the first group, we count how many values in the second group that are smaller than it. (If both values are equal, we count that as a tie, worth +0.5 to the tally.) This number of wins for the first group gives us a value $U$.\n",
    "\n",
    "It turns out that $U$ is approximately normally-distributed, given a large enough sample size. If we have $n_1$ data points in the first group and $n_2$ points in the second, then we have a total of $n_1 n_2$ matchups and an equivalent number of victory points to hand out. Under the null hypothesis, we should expect the number of wins to be evenly distributed between groups, and so the expected wins are $\\mu_U = \\frac{n_1 n_2}{2}$. The variability in the number of wins can be found to be the following equation (assuming no or few ties):\n",
    "\n",
    "$$ \n",
    "\\sigma_U = \\sqrt{\\frac{n_1n_2(n_1+n_2+1)}{12}}\n",
    "$$\n",
    "\n",
    "These $\\mu_U$ and $\\sigma_U$ values can then be used to compute a standard normal z-score, which generates a p-value. Implement this method of performing the rank-sum test in the cells below!\n",
    "\n",
    "- HINT: scipy stats' [`norm`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) class can be used to obtain p-values after computing a z-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranked_sum(x, y, alternative = 'two-sided'):\n",
    "    \"\"\"\n",
    "    Return a p-value for a ranked-sum test, assuming no ties.\n",
    "    \n",
    "    Input parameters:\n",
    "        x: 1-D array-like of data for first group\n",
    "        y: 1-D array-like of data for second group\n",
    "        alternative: type of test to perform, {'two-sided', less', 'greater'}\n",
    "    \n",
    "    Output value:\n",
    "        p: estimated p-value of test\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute U\n",
    "    u = \n",
    "    \n",
    "    # compute a z-score\n",
    "    n_1 = \n",
    "    n_2 = \n",
    "    mean_u = # expected value for U statistic\n",
    "    sd_u = # expected standard deviation for U statistic\n",
    "    z = # U value z-score\n",
    "    \n",
    "    # compute a p-value\n",
    "    if alternative == 'two-sided':\n",
    "        p = \n",
    "    if alternative == 'less':\n",
    "        p = \n",
    "    elif alternative == 'greater':\n",
    "        p = \n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/permutation_data.csv')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualization\n",
    "bin_borders = np.arange(0, data['time'].max()+400, 400)\n",
    "plt.hist(data[data['condition'] == 0]['time'], alpha = 0.5, bins = bin_borders)\n",
    "plt.hist(data[data['condition'] == 1]['time'], alpha = 0.5, bins = bin_borders)\n",
    "plt.legend(labels = ['control', 'experiment']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sum(data[data['condition'] == 0]['time'],\n",
    "           data[data['condition'] == 1]['time'],\n",
    "           alternative = 'greater')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank-Sum Test Notes\n",
    "\n",
    "For smaller sample sizes, something like the permutation test can be performed. After exhaustively checking the distribution of victories for every possible assignment of group labels to value, a p-value can be computed for how unusual the actually-observed $U$ was.\n",
    "\n",
    "Also, there already exists a function in the scipy stats package [`mannwhitneyu`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html) that performs the Mann Whitney U test. This function considers more factors than the implementation above, including a correction on the standard deviation for ties and a continuity correction (since we're approximating a discretely-valued distribution with a continuous one). In addition, the approach they take is computationally more efficient, based on the sum of value ranks (hence the rank-sum test name) rather than the matchups explanation provided above.\n",
    "\n",
    "Reference: [Wikipedia](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.mannwhitneyu(data[data['condition'] == 0]['time'],\n",
    "                   data[data['condition'] == 1]['time'],\n",
    "                   alternative = 'greater')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign Test\n",
    "\n",
    "The sign test also only uses the collected data to compute a test result. It only requires that there be paired values between two groups to compare, and tests whether one group's values tend to be higher than the other's.\n",
    "\n",
    "In the sign test, we don't care how large differences are between groups, only which group takes a larger value. So comparisons of 0.21 vs. 0.22 and 0.21 vs. 0.31 are both counted equally as a point in favor of the second group. This makes the sign test a fairly weak test, though also a test that can be applied fairly broadly. It's most useful when we have very few observations to draw from and can't make a good assumption of underlying distribution characteristics. For example, you might use a sign test as an additional check on click rates that have been aggregated on a daily basis.\n",
    "\n",
    "The count of victories for a particular group can be modeled with the binomial distribution. Under the null hypothesis, it is equally likely that either group has a larger value (in the case of a tie, we ignore the comparison): the binomial distribution's success parameter is $p = 0.5$. Implement the sign test in the function below!\n",
    "\n",
    "- HINT: scipy stats' [`binom`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html) class can be used to obtain p-values after computing the number of matchups and victories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_test(x, y, alternative = 'two-sided'):\n",
    "    \"\"\"\n",
    "    Return a p-value for a ranked-sum test, assuming no ties.\n",
    "    \n",
    "    Input parameters:\n",
    "        x: 1-D array-like of data for first group\n",
    "        y: 1-D array-like of data for second group\n",
    "        alternative: type of test to perform, {'two-sided', less', 'greater'}\n",
    "    \n",
    "    Output value:\n",
    "        p: estimated p-value of test\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute parameters\n",
    "    n = # number of matchups\n",
    "    k = # number of victories for first group\n",
    "\n",
    "    # compute a p-value\n",
    "    if alternative == 'two-sided':\n",
    "        p = \n",
    "    if alternative == 'less':\n",
    "        p = \n",
    "    elif alternative == 'greater':\n",
    "        p = \n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/signtest_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualization\n",
    "plt.plot(data['day'], data['control'])\n",
    "plt.plot(data['day'], data['exp'])\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Day of Experiment')\n",
    "plt.ylabel('Success rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_test(data['control'], data['exp'], 'less')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
