{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Size\n",
    "\n",
    "We can use the knowledge of our desired practical significance boundary to plan out our experiment. By knowing how many observations we need in order to detect our desired effect to our desired level of reliability, we can see how long we would need to run our experiment and whether or not it is feasible.\n",
    "\n",
    "Let's use the example from the video, where we have a baseline click-through rate of 10% and want to see a manipulation increase this baseline to 12%. How many observations would we need in each group in order to detect this change with power $1-\\beta = .80$ (i.e. detect the 2% absolute increase 80% of the time), at a Type I error rate of $\\alpha = .05$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Trial and Error\n",
    "\n",
    "One way we could solve this is through trial and error. Every sample size will have a level of power associated with it; testing multiple sample sizes will gradually allow us to narrow down the minimum sample size required to obtain our desired power level. This isn't a particularly efficient method, but it can provide an intuition for how experiment sizing works.\n",
    "\n",
    "Fill in the `power()` function below following these steps:\n",
    "\n",
    "1. Under the null hypothesis, we should have a critical value for which the Type I error rate is at our desired alpha level.\n",
    "  - `se_null`: Compute the standard deviation for the difference in proportions under the null hypothesis for our two groups. The base probability is given by `p_null`. Remember that the variance of the difference distribution is the sum of the variances for the individual distributions, and that _each_ group is assigned `n` observations.\n",
    "  - `null_dist`: To assist in re-use, this should be a [scipy norm object](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html). Specify the center and standard deviation of the normal distribution using the \"loc\" and \"scale\" arguments, respectively.\n",
    "  - `p_crit`: Compute the critical value of the distribution that would cause us to reject the null hypothesis. One of the methods of the `null_dist` object will help you obtain this value (passing in some function of our desired error rate `alpha`).\n",
    "2. The power is the proportion of the distribution under the alternative hypothesis that is past that previously-obtained critical value.\n",
    "  - `se_alt`: Now it's time to make computations in the other direction. This will be standard deviation of differences under the desired detectable difference. Note that the individual distributions will have different variances now: one with `p_null` probability of success, and the other with `p_alt` probability of success.\n",
    "  - `alt_dist`: This will be a scipy norm object like above. Be careful of the \"loc\" argument in this one. The way the `power` function is set up, it expects `p_alt` to be greater than `p_null`, for a positive difference.\n",
    "  - `beta`: Beta is the probability of a Type-II error, or the probability of failing to reject the null for a particular non-null state. That means you should make use of `alt_dist` and `p_crit` here!\n",
    "\n",
    "The second half of the function has already been completed for you, which creates a visualization of the distribution of differences for the null case and for the desired detectable difference. Use the cells that follow to run the function and observe the visualizations, and to test your code against a few assertion statements. Check the following page if you need help coming up with the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(p_null, p_alt, n, alpha = .05, plot = True):\n",
    "    \"\"\"\n",
    "    Compute the power of detecting the difference in two populations with \n",
    "    different proportion parameters, given a desired alpha rate.\n",
    "    \n",
    "    Input parameters:\n",
    "        p_null: base success rate under null hypothesis\n",
    "        p_alt : desired success rate to be detected, must be larger than\n",
    "                p_null\n",
    "        n     : number of observations made in each group\n",
    "        alpha : Type-I error rate\n",
    "        plot  : boolean for whether or not a plot of distributions will be\n",
    "                created\n",
    "    \n",
    "    Output value:\n",
    "        power : Power to detect the desired difference, under the null.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the power\n",
    "    se_null = \n",
    "    null_dist = \n",
    "    p_crit = \n",
    "    \n",
    "    se_alt = \n",
    "    alt_dist = \n",
    "    beta = \n",
    "    \n",
    "    if plot:\n",
    "        # Compute distribution heights\n",
    "        low_bound = null_dist.ppf(.01)\n",
    "        high_bound = alt_dist.ppf(.99)\n",
    "        x = np.linspace(low_bound, high_bound, 201)\n",
    "        y_null = null_dist.pdf(x)\n",
    "        y_alt = alt_dist.pdf(x)\n",
    "\n",
    "        # Plot the distributions\n",
    "        plt.plot(x, y_null)\n",
    "        plt.plot(x, y_alt)\n",
    "        plt.vlines(p_crit, 0, np.amax([null_dist.pdf(p_crit), alt_dist.pdf(p_crit)]),\n",
    "                   linestyles = '--')\n",
    "        plt.fill_between(x, y_null, 0, where = (x >= p_crit), alpha = .5)\n",
    "        plt.fill_between(x, y_alt , 0, where = (x <= p_crit), alpha = .5)\n",
    "        \n",
    "        plt.legend(['null','alt'])\n",
    "        plt.xlabel('difference')\n",
    "        plt.ylabel('density')\n",
    "        plt.show()\n",
    "    \n",
    "    # return power\n",
    "    return (1 - beta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power(.1, .12, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(power(.1, .12, 1000, plot = False), 0.4412, atol = 1e-4)\n",
    "assert np.isclose(power(.1, .12, 3000, plot = False), 0.8157, atol = 1e-4)\n",
    "assert np.isclose(power(.1, .12, 5000, plot = False), 0.9474, atol = 1e-4)\n",
    "print('You should see this message if all the assertions passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Analytic Solution\n",
    "\n",
    "Now that we've got some intuition for power by using trial and error, we can now approach a closed-form solution for computing a minimum experiment size. The key point to notice is that, for an $\\alpha$ and $\\beta$ both < .5, the critical value for determining statistical significance will fall between our null click-through rate and our alternative, desired click-through rate. So, the difference between $p_0$ and $p_1$ can be subdivided into the distance from $p_0$ to the critical value $p^*$ and the distance from $p^*$ to $p_1$.\n",
    "\n",
    "<img src= 'images/ExpSize_Power.png'>\n",
    "\n",
    "Those subdivisions can be expressed in terms of the standard error and the z-scores:\n",
    "\n",
    "$$p^* - p_0 = z_{1-\\alpha} SE_{0},$$\n",
    "$$p_1 - p^* = -z_{\\beta} SE_{1};$$\n",
    "\n",
    "$$p_1 - p_0 = z_{1-\\alpha} SE_{0} - z_{\\beta} SE_{1}$$\n",
    "\n",
    "In turn, the standard errors can be expressed in terms of the standard deviations of the distributions, divided by the square root of the number of samples in each group:\n",
    "\n",
    "$$SE_{0} = \\frac{s_{0}}{\\sqrt{n}},$$\n",
    "$$SE_{1} = \\frac{s_{1}}{\\sqrt{n}}$$\n",
    "\n",
    "Substituting these values in and solving for $n$ will give us a formula for computing a minimum sample size to detect a specified difference, at the desired level of power:\n",
    "\n",
    "$$n = \\lceil \\big(\\frac{z_{\\alpha} s_{0} - z_{\\beta} s_{1}}{p_1 - p_0}\\big)^2 \\rceil$$\n",
    "\n",
    "where $\\lceil ... \\rceil$ represents the ceiling function, rounding up decimal values to the next-higher integer. Implement the necessary variables in the function below, and test them with the cells that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_size(p_null, p_alt, alpha = .05, beta = .20):\n",
    "    \"\"\"\n",
    "    Compute the minimum number of samples needed to achieve a desired power\n",
    "    level for a given effect size.\n",
    "    \n",
    "    Input parameters:\n",
    "        p_null: base success rate under null hypothesis\n",
    "        p_alt : desired success rate to be detected\n",
    "        alpha : Type-I error rate\n",
    "        beta  : Type-II error rate\n",
    "    \n",
    "    Output value:\n",
    "        n : Number of samples required for each group to obtain desired power\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get necessary z-scores and standard deviations (@ 1 obs per group)\n",
    "    z_null = \n",
    "    z_alt  = \n",
    "    sd_null = \n",
    "    sd_alt  = \n",
    "    \n",
    "    # Compute and return minimum sample size\n",
    "    n = \n",
    "    return np.ceil(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_size(.1, .12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(experiment_size(.1, .12), 2863)\n",
    "print('You should see this message if the assertion passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Interpretation\n",
    "\n",
    "The example explored above is a one-tailed test, with the alternative value greater than the null. The power computations performed in the first part will _not_ work if the alternative proportion is less than the null, e.g. detecting a proportion parameter of 0.88 against a null of 0.9. You might want to try to rewrite the code to handle that case! The same issue should not show up for the second approach, where we directly compute the sample size.\n",
    "\n",
    "If you find that you need to do a two-tailed test, you should pay attention to two main things. First of all, the \"alpha\" parameter needs to account for the fact that the rejection region is divided into two areas. Secondly, you should perform the computation based on the worst-case scenario, the alternative case with the highest variability. Since, for the binomial, variance is highest when $p = .5$, decreasing as $p$ approaches 0 or 1, you should choose the alternative value that is closest to .5 as your reference when computing the necessary sample size.\n",
    "\n",
    "Note as well that the above methods only perform sizing for _statistical significance_, and do not take into account _practical significance_. One thing to realize is that if the true size of the experimental effect is the same as the desired practical significance level, then it's a coin flip whether the mean will be above or below the practical significance bound. This also doesn't even consider how a confidence interval might interact with that bound. In a way, experiment sizing is a way of checking on whether or not you'll be able to get what you _want_ from running an experiment, rather than checking if you'll get what you _need_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Approaches\n",
    "\n",
    "There are also tools and Python packages that can also help with sample sizing decisions, so you don't need to solve for every case on your own. The sample size calculator [here](http://www.evanmiller.org/ab-testing/sample-size.html) is applicable for proportions, and provides the same results as the methods explored above. (Note that the calculator assumes a two-tailed test, however.) Python package \"statsmodels\" has a number of functions in its [`power` module](https://www.statsmodels.org/stable/stats.html#power-and-sample-size-calculations) that perform power and sample size calculations. Unlike previously shown methods, differences between null and alternative are parameterized as an effect size (standardized difference between group means divided by the standard deviation). Thus, we can use these functions for more than just tests of proportions. If we want to do the same tests as before, the [`proportion_effectsize`](http://www.statsmodels.org/stable/generated/statsmodels.stats.proportion.proportion_effectsize.html) function computes [Cohen's h](https://en.wikipedia.org/wiki/Cohen%27s_h) as a measure of effect size. As a result, the output of the statsmodel functions will be different from the result expected above. This shouldn't be a major concern since in most cases, you're not going to be stopping based on an exact number of observations. You'll just use the value to make general design decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of using statsmodels for sample size calculation\n",
    "from statsmodels.stats.power import NormalIndPower\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "\n",
    "# leave out the \"nobs\" parameter to solve for it\n",
    "NormalIndPower().solve_power(effect_size = proportion_effectsize(.12, .1), alpha = .05, power = 0.8,\n",
    "                             alternative = 'larger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
